import os
import re
import time
import random
import requests
import torch
import pdfplumber
from typing import List, Optional, Dict, Tuple
from urllib.parse import urljoin
from io import BytesIO
from datetime import datetime
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer, util

# ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ í•¨ìˆ˜ Import
from direct_quote import extract_span
from find_original import translate_ko_to_en
from per_name import get_wikidata_english_name

# ==========================================================
# ëª¨ë¸ ë° ì„¸ì…˜ ë¡œë”©
# ==========================================================

# 0-a) SPAN ë§¤ì¹­ìš© SentenceTransformer (ìœ ì‚¬ë„ ì „ìš©)
sim_model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")

BASE_DOMAINS = [
    "site:whitehouse.gov",
    "site:congress.gov",
    "site:rollcall.com",
    "site:millercenter.org",
    "site:un.org",
        # íŠ¸ëŸ¼í”„/ë¯¸êµ­ ì •ì¹˜ ì—°ì„¤Â·ì¸í„°ë·° transcript ë§ì´ ìˆëŠ” ê³³ë“¤
    "site:factba.se",
    "site:foxnews.com",
    "site:c-span.org",
    "site:abcnews.go.com",
    "site:nbcnews.com",
    "site:cnn.com",
]

SESSION = requests.Session()
SESSION.headers.update({
    "User-Agent": "Mozilla/5.0 (compatible; QuoteContextBot/1.0; +https://example.org/bot)",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
})


def contains_korean(text: str) -> bool:
    return bool(re.search(r"[ê°€-í£]", text))

def is_valid_page(url: str, timeout: int = 12) -> bool:
    try:
        r = SESSION.get(url, timeout=timeout, allow_redirects=True)
        if r.status_code != 200:
            return False
        ct = (r.headers.get("Content-Type") or "").lower()
        if not ("text/html" in ct or "application/xhtml+xml" in ct):
            return False
        return len(r.text.strip()) > 500
    except requests.RequestException:
        return False

def google_cse_search(
    q: str,
    num: int = 10,
    start: int = 1,
    lr: Optional[str] = None,
    hl: str = "en",
    gl: str = "us",
    safe: Optional[str] = None,
    retries: int = 3,
    backoff: float = 1.4,
):
    google_api_key = os.getenv("GOOGLE_API_KEY")
    google_cse_cx = os.getenv("GOOGLE_CSE_CX")
    assert google_api_key and google_cse_cx, "í™˜ê²½ë³€ìˆ˜ GOOGLE_API_KEY / GOOGLE_CSE_CX ë¥¼ ì„¤ì •í•˜ì„¸ìš”."

    params = {
        "key": google_api_key,
        "cx": google_cse_cx,
        "q": q,
        "num": max(1, min(10, int(num))),
        "start": max(1, min(91, int(start))),
        "hl": hl,
        "gl": gl,
    }
    if lr:
        params["lr"] = lr
    if safe in ("active", "off"):
        params["safe"] = safe

    url = "https://www.googleapis.com/customsearch/v1"

    for attempt in range(retries):
        try:
            resp = SESSION.get(url, params=params, timeout=5)
                        # ğŸ” ì—¬ê¸°ì„œ ì‹¤ì œ ì‘ë‹µì„ ì°ì–´ë³´ëŠ” ê²Œ í•µì‹¬
            print("\n[DEBUG] CSE ìš”ì²­ ì‹œë„:", attempt + 1)
            print("[DEBUG] status:", resp.status_code)
            print("[DEBUG] url:", resp.url)
            #print("[DEBUG] body ì•ë¶€ë¶„:", resp.text[:300])

            if resp.status_code == 200:
                return resp.json()
            if resp.status_code in (429, 500, 502, 503, 504):
                sleep_s = (backoff ** attempt) + random.uniform(0, 0.25)
                time.sleep(sleep_s)
                continue
            resp.raise_for_status()
        except requests.RequestException:
            sleep_s = (backoff ** attempt) + random.uniform(0, 0.25)
            time.sleep(sleep_s)
            continue

    return {"items": []}


def collect_candidates_google_cse(
    query: str,
    top_per_domain: int = 3,
    use_siteSearch: bool = True,
    safe: Optional[str] = None,
    domain_list: Optional[List[str]] = None,  # ğŸ”¹ ì¶”ê°€
):
    """
    ğŸ”’ ë„ë©”ì¸ ì œí•œì„ ë‹¤ì‹œ ì ìš©í•œ ë²„ì „
    â†’ BASE_DOMAINS ë¦¬ìŠ¤íŠ¸ ì•ˆì— ì •ì˜ëœ ë„ë©”ì¸ë§Œ ê²€ìƒ‰
    """
    candidates = []
    seen = set()

    is_ko = contains_korean(query)
    lr = "lang_ko" if is_ko else None
    hl = "ko" if is_ko else "en"
    gl = "kr" if is_ko else "us"

     # ğŸ”¹ ê²€ìƒ‰ì— ì‚¬ìš©í•  ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸ ê²°ì •
    domains = domain_list if domain_list is not None else BASE_DOMAINS

    for site_filter in domains:   # â† ì´ ì¤„ë§Œ ìˆ˜ì •
        sub_query = f"{query} {site_filter}"
        want = top_per_domain
        start = 1

        while want > 0:
            per_req = min(10, want)

            data = google_cse_search(
                q=sub_query,
                num=per_req,
                start=start,
                lr=lr,
                hl=hl,
                gl=gl,
                safe=safe,
            )
            items = data.get("items", []) or []
            if not items:
                break

            for it in items:
                url = it.get("link") or it.get("formattedUrl")
                if not url or url in seen:
                    continue

                # site í•„í„°ë¥¼ ë§Œì¡±í•˜ëŠ” í˜ì´ì§€ì¸ì§€ í™•ì¸
                if not is_valid_page(url):
                    continue

                candidates.append({
                    "domain": site_filter.replace("site:", ""),
                    "title": it.get("title", ""),
                    "url": url,
                    "snippet": it.get("snippet", ""),
                })
                seen.add(url)
                want -= 1

                if want == 0:
                    break

            start += per_req
            if start > 91:
                break
            time.sleep(0.2)

    return candidates


def html_to_text(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()
    text = soup.get_text(" ")
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def split_into_sentences(text: str, is_ko: Optional[bool] = None) -> List[str]:
    """
    ì˜ì–´/í•œêµ­ì–´ ëª¨ë‘ì—ì„œ ë¬´ë‚œí•˜ê²Œ ì“¸ ìˆ˜ ìˆëŠ” ë¬¸ì¥ ë¶„ë¦¬ê¸°
    """
    if is_ko is None:
        is_ko = bool(re.search(r"[ê°€-í£]", text))

    rough = re.split(r"(?<=[.!?])\s+", text)
    sentences = []

    for s in rough:
        s = s.strip()
        if not s:
            continue

        if is_ko:
            if len(s) < 10:
                continue
        else:
            if len(s) < 20:
                continue

        sentences.append(s)

    return sentences


def extract_pdf_url_from_html(html: str, base_url: str) -> Optional[str]:
    """UN í˜ì´ì§€ì²˜ëŸ¼ PDFë¥¼ iframe/aë¡œ embedí•œ ê²½ìš° PDF ë§í¬ ì¶”ì¶œ"""
    soup = BeautifulSoup(html, "html.parser")

    iframe = soup.find("iframe")
    if iframe and iframe.get("src"):
         src = iframe["src"]
         # ì§„ì§œ PDFì¸ ê²½ìš°ì—ë§Œ PDFë¡œ ì²˜ë¦¬
         if ".pdf" in src.lower():
             return urljoin(base_url, src)

    for a in soup.find_all("a", href=True):
        href = a["href"]
        if ".pdf" in href.lower():
            return urljoin(base_url, href)

    return None


def extract_text_from_pdf_url(pdf_url: str) -> Optional[str]:
    """PDF URLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        r = SESSION.get(pdf_url, timeout=20)
        if r.status_code != 200:
            print(f"[WARN] PDF ìš”ì²­ ì‹¤íŒ¨: {pdf_url}, status={r.status_code}")
            return None
    except Exception as e:
        print(f"[WARN] PDF ìš”ì²­ ì—ëŸ¬: {pdf_url}, {e}")
        return None

    pdf_file = BytesIO(r.content)
    text_chunks = []

    try:
        with pdfplumber.open(pdf_file) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text() or ""
                text_chunks.append(page_text)
    except Exception as e:
        print(f"[WARN] PDF íŒŒì‹± ì—ëŸ¬: {pdf_url}, {e}")
        return None

    text = "\n".join(text_chunks)
    text = re.sub(r"\s+", " ", text)

    try:
        text = bytes(text, "utf-8").decode("utf-8", "ignore")
    except Exception:
        pass

    return text.strip() or None


def semantic_similarity(text1: str, text2: str) -> float:
    """
    text1, text2ë¥¼ ê°™ì€ SentenceTransformer(sim_model)ë¡œ ì„ë² ë”©í•´ì„œ
    ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë°˜í™˜
    """
    with torch.no_grad():
        embeddings = sim_model.encode(
            [text1, text2],
            convert_to_tensor=True,
            normalize_embeddings=True,  # L2 ì •ê·œí™”
        )
        emb1, emb2 = embeddings[0], embeddings[1]
        sim = util.cos_sim(emb1, emb2)  # shape: (1, 1)
        return float(sim.item())

def find_best_match_span_in_snippet(
    quote_text: str,
    snippet_text: str,
    url: str,
    num_before: int = 1,
    num_after: int = 1,
):
    """
    Google CSE ìŠ¤ë‹ˆí« í…ìŠ¤íŠ¸ ë‚´ì—ì„œ quote_textì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ë§¥ Spanì„ ì°¾ëŠ”ë‹¤.
    ì´ í•¨ìˆ˜ëŠ” ì›¹ í˜ì´ì§€ ì „ì²´ë¥¼ ë¡œë“œí•˜ì§€ ì•Šì•„ ì†ë„ê°€ ë§¤ìš° ë¹ ë¥´ë‹¤.
    """
    if not snippet_text:
        return None

    # ìŠ¤ë‹ˆí« í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (Span í›„ë³´)
    sentences = split_into_sentences(snippet_text, is_ko=False)

    if not sentences:
        return None

    n = len(sentences)

    # 1) ë¬¸ì¥ ë‹¨ìœ„ ìœ ì‚¬ë„ ê³„ì‚° (SBERT ë°°ì¹˜ ì¸ì½”ë”©)
    try:
        with torch.no_grad():
            # (1) quote í•˜ë‚˜ë§Œ ì¸ì½”ë”©
            quote_emb = sim_model.encode(
                [quote_text],
                convert_to_tensor=True,
                normalize_embeddings=True,
            )[0]  # (d,)

            # (2) ìŠ¤ë‹ˆí« ë¬¸ì¥ ì „ì²´ë¥¼ í•œ ë²ˆì— ì¸ì½”ë”©
            sent_embs = sim_model.encode(
                sentences,
                convert_to_tensor=True,
                normalize_embeddings=True,
            )  # (m, d)

            # (3) ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë²¡í„° (1 x m)
            sims = util.cos_sim(quote_emb, sent_embs)[0]  # (m,)

            best_local_idx = int(torch.argmax(sims).item())
            best_score = float(sims[best_local_idx].item())
    except Exception as e:
        print(f"[WARN] SBERT ì¸ì½”ë”©/ìœ ì‚¬ë„ ê³„ì‚° ì—ëŸ¬: {e}")
        return None


    # 2) best_idx ê¸°ì¤€ìœ¼ë¡œ span êµ¬ì„± (contextìš©)
    # ìŠ¤ë‹ˆí«ì´ ì§§ìœ¼ë¯€ë¡œ num_before/num_afterëŠ” ìŠ¤ë‹ˆí« ë‚´ì—ì„œë§Œ ì ìš©ë¨
    span_text, s_idx, e_idx = extract_span(
        sentences,
        best_local_idx,
        num_before=num_before,
        num_after=num_after,
        join_with=" ",
    )

    return {
        "url": url,
        "best_sentence": sentences[best_local_idx],
        "best_score": best_score,
        "span_text": span_text,
        "span_start_idx": s_idx,
        "span_end_idx": e_idx,
    }

def find_best_span_from_candidates_debug(
    quote_en: str,
    candidates: List[Dict],
    num_before: int = 1,
    num_after: int = 1,
    min_score: float=0.4,
):
    """
    ì—¬ëŸ¬ í›„ë³´ URL(candidates)ì— ëŒ€í•´:
      - ê° URLì—ì„œ quote_enê³¼ ê°€ì¥ ìœ ì‚¬í•œ spanì„ ì°¾ê³ 
      - best_scoreê°€ min_score ì´ìƒì¸ ê²ƒ ì¤‘ì—ì„œ
      - ì „ì—­ ìµœê³  ì ìˆ˜ë¥¼ ê°–ëŠ” span í•˜ë‚˜ë¥¼ ê³¨ë¼ì„œ ë°˜í™˜.

    ë°˜í™˜ í˜•ì‹ì€ find_best_match_span_in_pageì™€ ë™ì¼:
      {
        "url": ...,
        "best_sentence": ...,
        "best_score": ...,
        "span_text": ...,
        "span_start_idx": ...,
        "span_end_idx": ...,
      }
    ëª» ì°¾ìœ¼ë©´(None ë˜ëŠ” min_score ë¯¸ë§Œ) â†’ None
    """
    best_global = None

    for cand in candidates:
        url = cand.get("url")
        snippet = cand.get("snippet")
        if not url:
            continue

        try:
          # ğŸ’¡ ìˆ˜ì •ëœ ë¶€ë¶„: ì›¹ í˜ì´ì§€ ì „ì²´ ë¡œë“œ ëŒ€ì‹  ìŠ¤ë‹ˆí« ì‚¬ìš©
                span_res = find_best_match_span_in_snippet(
                quote_text=quote_en,
                snippet_text=snippet,
                url=url,
                num_before=num_before,
                num_after=num_after,
            )
        except Exception as e:
            print(f"[WARN] span ì¶”ì¶œ ì¤‘ ì—ëŸ¬ (url={url}, ìŠ¤ë‹ˆí« ì‚¬ìš©): {e}")
            continue

        if not span_res:
            continue

        score = span_res.get("best_score", -1.0)

        if score < min_score:
            # ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ìŠ¤í‚µ
            continue

        if (best_global is None) or (score > best_global["best_score"]):
            best_global = span_res

    return best_global

def resolve_person_name_en(name_ko: str) -> str:
    """
    ì¸ë¬¼ ì´ë¦„ ì˜ì–´í™”:
    1) Wikidataì—ì„œ ì˜ì–´ ë¼ë²¨ ì°¾ê¸°
    2) ì‹¤íŒ¨í•˜ë©´ ê¸°ê³„ë²ˆì—­(koâ†’en)
    3) ë²ˆì—­ë„ ì‹¤íŒ¨í•˜ë©´ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
    """
    # 1) Wikidata
    info = get_wikidata_english_name(name_ko)
    if isinstance(info, dict) and info.get("en"):
        return info["en"]

    # 2) ë²ˆì—­ fallback
    try:
        return translate_ko_to_en(name_ko)
    except Exception:
        # 3) ìµœì¢… fallback: ê·¸ëƒ¥ í•œê¸€ ê·¸ëŒ€ë¡œ
        return name_ko


def generate_search_query(
    entities_by_type: Dict[str, List[str]],
    keywords: List[Tuple[str, float]],
    top_k: int = 3,
    quote_sentence: Optional[str] = None,
    article_date: Optional[str] = None,   # ğŸ”¹ ê¸°ì‚¬ ë‚ ì§œ (YYYY-MM-DD)
    rollcall_mode: bool = False,          # ğŸ”¹ rollcall.com ì „ìš© ëª¨ë“œ
    use_wikidata: bool = True,
) -> Dict[str, Optional[str]]:

    """
    ê¸°ë³¸ ëª¨ë“œ:
      - PER: Wikidata or ë²ˆì—­ â†’ ì˜ì–´ ì´ë¦„
      - LOC: ê°œë³„ ë²ˆì—­ í›„ í•œë‘ ë‹¨ì–´ë§Œ ì‚¬ìš©
      - í‚¤ì›Œë“œ: ê°œë³„ ë²ˆì—­ í›„ ì§§ê²Œ ì‚¬ìš©
      - ì¸ìš©ë¬¸: ë²ˆì—­ í›„ **ì „ì²´ ë¬¸ì¥** ì‚¬ìš©

    rollcall_mode=True ì¼ ë•Œ:
      - ê²€ìƒ‰ ì¿¼ë¦¬ = [ë°œí™”ì ì˜ì–´] + [ê¸°ì‚¬ ë‚ ì§œ ì˜ì–´] + [í‚¤ì›Œë“œ ì˜ì–´ 1ê°œ]
    """

    # 1) PER (speaker)
    per_list = entities_by_type.get("PER", [])
    if not per_list:
        return {"ko": None, "en": None}

    speaker_ko = per_list[0]
    if use_wikidata:
        # Wikidata â†’ ì‹¤íŒ¨ ì‹œ ë²ˆì—­ê¹Œì§€ resolve_person_name_en ì•ˆì—ì„œ ì²˜ë¦¬
        speaker_en = resolve_person_name_en(speaker_ko)
    else:
        # Wikidata ì•ˆ ì“°ê² ë‹¤ê³  í•œ ê²½ìš°ë„ ê·¸ëƒ¥ ë²ˆì—­ìœ¼ë¡œ ì²˜ë¦¬
        try:
            speaker_en = translate_ko_to_en(speaker_ko)
        except Exception:
            speaker_en = speaker_ko


    # 2) LOC: ê°œë³„ ë²ˆì—­ + ì§§ê²Œ ìë¥´ê¸°
    loc_list = entities_by_type.get("LOC", [])[:2]
    locs_ko = " ".join(loc_list)

    locs_en_tokens: List[str] = []
    for loc in loc_list:
        try:
            loc_en_full = translate_ko_to_en(loc)  # ex) "Russia", "Ukraine"
            loc_en_first = loc_en_full.split(",")[0]  # ì½¤ë§ˆ ì•ê¹Œì§€ë§Œ
            loc_en_first = " ".join(loc_en_first.split()[:2])  # ë‹¨ì–´ 1~2ê°œë§Œ
            if loc_en_first:
                locs_en_tokens.append(loc_en_first)
        except Exception:
            continue

    # 3) í‚¤ì›Œë“œ: ê°œë³„ ë²ˆì—­ + ì§§ê²Œ
    top_kws_ko = [kw for kw, _ in keywords[:top_k]]
    kws_en_tokens: List[str] = []
    for kw_ko in top_kws_ko:
        try:
            kw_en_full = translate_ko_to_en(kw_ko)
            kw_en_trim = " ".join(kw_en_full.split()[:3])  # ì• 2~3ë‹¨ì–´ë§Œ
            if kw_en_trim:
                kws_en_tokens.append(kw_en_trim)
        except Exception:
            continue


    # 4) ì¸ìš©ë¬¸: **ì „ì²´ ë¬¸ì¥ ì‚¬ìš©**
    quote_en_full: Optional[str] = None
    if quote_sentence:
        try:
            quote_en_full = translate_ko_to_en(quote_sentence)
        except Exception:
            quote_en_full = None
        # rollcall.com ì „ìš© ëª¨ë“œ
    if rollcall_mode and article_date:
        # ê¸°ì‚¬ ë‚ ì§œë¥¼ ì˜ì–´ í¬ë§· (ì˜ˆ: November 02 2025)ë¡œ ë³€í™˜
        try:
            dt = datetime.strptime(article_date, "%Y-%m-%d")
            date_en = dt.strftime("%B %d %Y")
        except Exception:
            date_en = article_date

        # í‚¤ì›Œë“œ ëª…ì‚¬ í•˜ë‚˜: ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ê°€ì¥ ìƒìœ„ í‚¤ì›Œë“œ 1ê°œ ì‚¬ìš©
        kw_ko_main = top_kws_ko[0] if top_kws_ko else ""
        kw_en_main = ""
        if kw_ko_main:
            try:
                kw_en_full = translate_ko_to_en(kw_ko_main)
                kw_en_main = kw_en_full.split()[0]  # ì²« ë‹¨ì–´ë§Œ
            except Exception:
                kw_en_main = kw_ko_main

        parts_en = [speaker_en]
        if date_en:
            parts_en.append(date_en)
        if kw_en_main:
            parts_en.append(kw_en_main)

        query_en = " ".join(parts_en).strip()

        parts_ko = [speaker_ko]
        parts_ko.append(article_date)
        if kw_ko_main:
            parts_ko.append(kw_ko_main)
        query_ko = " ".join(parts_ko).strip()

        return {
            "ko": query_ko or None,
            "en": query_en or None,
        }
    # ì—¬ê¸°ê¹Œì§€ rollcall ëª¨ë“œ, ì•„ë˜ëŠ” ê¸°ì¡´ ê¸°ë³¸ ëª¨ë“œ

    # 5) EN ì¿¼ë¦¬ í† í° í•©ì¹˜ê¸°
    query_en_tokens: List[str] = [speaker_en]
    query_en_tokens += locs_en_tokens
    query_en_tokens += kws_en_tokens
    if quote_en_full:
        query_en_tokens.append(quote_en_full)

    query_en = " ".join(query_en_tokens).strip()

    # 6) KO ì¿¼ë¦¬ëŠ” ë””ë²„ê¹…/ë¡œê·¸ìš©
    query_ko_parts = [speaker_ko]
    if locs_ko:
        query_ko_parts.append(locs_ko)
    if top_kws_ko:
        query_ko_parts.append(" ".join(top_kws_ko))
    if quote_sentence:
        query_ko_parts.append(quote_sentence)
    query_ko = " ".join(query_ko_parts).strip()

    return {
        "ko": query_ko or None,
        "en": query_en or None,
    }