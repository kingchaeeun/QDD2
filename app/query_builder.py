"""
Search-query construction utilities.
"""

import logging
import re
from datetime import datetime
from typing import Dict, List, Optional, Tuple

from app.name_resolution import resolve_person_name_en
from app.translation import translate_ko_to_en

logger = logging.getLogger(__name__)

def _format_date_en(article_date: Optional[str]) -> Tuple[Optional[str], Optional[str]]:
    """
    article_dateë¥¼ ë¬¸ìì—´ë¡œ ë°›ì•„ì„œ
    - ì›ë³¸ ë¬¸ìì—´ (article_date_str)
    - ì˜ì–´ í¬ë§·(ì˜ˆ: November 30, 2025) ì„ íŠœí”Œë¡œ ë°˜í™˜

    ì¸ì‹ ê°€ëŠ¥í•œ í¬ë§·: YYYY-MM-DD, YYYY.MM.DD, YYYY/MM/DD
    """
    if article_date is None:
        return None, None

    s = str(article_date).strip()
    if not s:
        return None, None

    dt = None
    for fmt in ("%Y-%m-%d", "%Y.%m.%d", "%Y/%m/%d"):
        try:
            dt = datetime.strptime(s, fmt)
            break
        except ValueError:
            continue

    if dt is None:
        # ëª» íŒŒì‹±í•˜ë©´ ê·¸ëƒ¥ ì›ë³¸ì„ ê·¸ëŒ€ë¡œ ì“°ë„ë¡
        return s, s

    # ì›í•˜ëŠ” í¬ë§·: November 30, 2025  (ì‰¼í‘œ í¬í•¨)
    date_en = dt.strftime("%B %d, %Y")
    return s, date_en


def _normalize_token(tok: str) -> str:
    """Normalize token for deduplication: lowercase, strip punctuation/extra spaces."""
    normalized = re.sub(r"[^\w\s]", " ", tok).lower()
    return " ".join(normalized.split()).strip()


def _dedupe_preserve(seq: List[str]) -> List[str]:
    """Remove duplicates while preserving order and ignoring empty tokens (punct/space-insensitive)."""
    seen = set()
    out: List[str] = []
    for item in seq:
        if not item:
            continue
        norm = _normalize_token(item)
        if not norm or norm in seen:
            continue
        seen.add(norm)
        out.append(item)
    return out

def _select_rollcall_focus_entity(
    entities: Optional[List[Dict[str, str]]],
    speaker_ko: Optional[str] = None,
) -> Tuple[str, str]:
    """
    Rollcall ëª¨ë“œì—ì„œ ì‚¬ìš©í•  'í¬ì»¤ìŠ¤ ì—”í‹°í‹°' 1ê°œ ì„ íƒ.

    - ìŠ¤í”¼ì»¤ì™€ ê²¹ì¹˜ëŠ” PSëŠ” ì œì™¸
    - ìš°ì„ ìˆœìœ„: LC > OG > PS
    - ê°™ì€ textëŠ” ë¹ˆë„/ê¸¸ì´ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ì½”ì–´ë§
    - ë°˜í™˜: (ko_text, en_text) (ì—†ìœ¼ë©´ ("",""))
    """
    if not entities:
        return "", ""

    # monologg/koelectra-base-v3-naver-ner ê¸°ì¤€
    LABEL_PRIORITY = {
        "LC": 3,  # ì¥ì†Œ
        "OG": 2,  # ì¡°ì§
        "PS": 1,  # ì‚¬ëŒ(ìŠ¤í”¼ì»¤ ì œì™¸)
    }

    stats: Dict[str, Dict[str, object]] = {}

    for ent in entities:
        text_val = (ent.get("text") or "").strip()
        if not text_val:
            continue

        raw_label = (
            ent.get("label")
            or ent.get("tag")
            or ent.get("ner")
            or ""
        )
        label = raw_label.replace("B-", "").replace("I-", "")

        # ê´€ì‹¬ ì—†ëŠ” ë ˆì´ë¸”ì€ ì œì™¸
        if label not in LABEL_PRIORITY:
            continue

        # ìŠ¤í”¼ì»¤ ì´ë¦„ê³¼ ê²¹ì¹˜ëŠ” PSëŠ” ì œì™¸
        if label == "PS" and speaker_ko:
            if text_val in speaker_ko or speaker_ko in text_val:
                continue

        key = text_val  # ê°™ì€ textëŠ” í•˜ë‚˜ë¡œ ë¬¶ìŒ
        entry = stats.get(key)
        if entry is None:
            stats[key] = {
                "label": label,
                "count": 1,
                "len": len(text_val),
                "translated": ent.get("translated", text_val),
            }
        else:
            entry["count"] = int(entry["count"]) + 1
            entry["len"] = max(int(entry["len"]), len(text_val))

    if not stats:
        return "", ""

    # ìŠ¤ì½”ì–´: ë ˆì´ë¸” ìš°ì„ ìˆœìœ„ -> ë“±ì¥ ë¹ˆë„ -> ê¸¸ì´
    def _score_item(item):
        text, info = item
        label = info["label"]
        count = info["count"]
        length = info["len"]
        base = LABEL_PRIORITY.get(label, 0)
        return (base, count, length)

    best_text, best_info = sorted(
        stats.items(),
        key=_score_item,
        reverse=True,
    )[0]

    return best_text, str(best_info.get("translated", best_text))


def generate_search_query(
    entities_by_type: Dict[str, List[str]],
    keywords: List[Tuple[str, float]],
    top_k: int = 3,
    quote_sentence: Optional[str] = None,
    article_date: Optional[str] = None,  # YYYY-MM-DD
    rollcall_mode: bool = False,
    use_wikidata: bool = True,
    # ğŸ”¥ ì¶”ê°€: NER ì—”í‹°í‹° ì›ë³¸ ë¦¬ìŠ¤íŠ¸ (text/label/translated ë“± ë“¤ì–´ìˆëŠ” dict ë¦¬ìŠ¤íŠ¸)
    entities: Optional[List[Dict[str, str]]] = None,
) -> Dict[str, Optional[str]]:
    """
    Build Korean/English search queries using entities + keywords.

    rollcall_mode=True:
        query_ko/en = [speaker] [article_date] [NER ê³ ìœ ëª…ì‚¬ 1ê°œ (PS/OG/LC)]
    default:
        query = speaker + location tokens + keyword tokens + optional quoted sentence
    """
    article_date_str, date_en = _format_date_en(article_date)
    per_list = entities_by_type.get("PER", [])
    if not per_list:
        return {"ko": None, "en": None}

    speaker_ko = per_list[0]
    if use_wikidata:
        speaker_en = resolve_person_name_en(speaker_ko)
    else:
        try:
            speaker_en = translate_ko_to_en(speaker_ko)
        except Exception:
            speaker_en = speaker_ko

    # LOCëŠ” ì¼ë°˜ ëª¨ë“œì—ì„œë§Œ ì‚¬ìš©í•  ê±°ë¼ ê·¸ëŒ€ë¡œ ë‘ 
    loc_list = entities_by_type.get("LOC", [])[:2]
    loc_list = _dedupe_preserve(loc_list)
    locs_ko = " ".join(loc_list)
    locs_en_tokens: List[str] = []
    for loc in loc_list:
        try:
            loc_en_full = translate_ko_to_en(loc)
            loc_en_first = loc_en_full.split(",")[0]
            loc_en_first = " ".join(loc_en_first.split()[:2])
            if loc_en_first:
                locs_en_tokens.append(loc_en_first)
        except Exception:
            logger.warning("Location translation failed, falling back to original: %s", loc)
            locs_en_tokens.append(loc)

    top_kws_ko = [kw for kw, _ in keywords[:top_k]]
    top_kws_ko = _dedupe_preserve(top_kws_ko)
    kws_en_tokens: List[str] = []
    for kw_ko in top_kws_ko:
        try:
            kw_en_full = translate_ko_to_en(kw_ko)
            kw_en_trim = " ".join(kw_en_full.split()[:3])
            if kw_en_trim:
                kws_en_tokens.append(kw_en_trim)
        except Exception:
            logger.warning("Keyword translation failed, falling back to original: %s", kw_ko)
            kws_en_tokens.append(kw_ko)

    quote_en_full: Optional[str] = None
    if quote_sentence:
        try:
            quote_en_full = translate_ko_to_en(quote_sentence)
        except Exception:
            quote_en_full = None

    # =========================
    # 1) Rollcall ëª¨ë“œ ì „ìš© ë¸”ë¡
    # =========================
    if rollcall_mode and article_date is not None:

        # article_date_str, date_en ëŠ” í•¨ìˆ˜ ì‹œì‘ë¶€ì—ì„œ _format_date_en ë¡œ ì´ë¯¸ ê³„ì‚°ë¨
        # article_date_str: ì›ë³¸ (ì˜ˆ: "2025.11.30")
        # date_en        : "November 30, 2025"

        # --- (ì„ íƒ) speaker_en ì •ì œ í•¨ìˆ˜ ---
        def normalize_name_en(name: str, max_words: int = 3) -> str:
            import re as _re
            name = _re.sub(r"[^A-Za-z\s]", " ", str(name))
            name = _re.sub(r"\s+", " ", name).strip()
            parts = name.split()
            if not parts:
                return ""
            return " ".join(parts[:max_words])

        # ===========================
        # ìµœì¢… ì¿¼ë¦¬ êµ¬ì„± (EN / KO)
        # ===========================
        # EN: ë°œí™”ì + ë‚ ì§œ
        parts_en = []
        if speaker_en:
            speaker_en_clean = normalize_name_en(speaker_en, max_words=3)
            if speaker_en_clean:
                parts_en.append(speaker_en_clean)
        if date_en:
            parts_en.append(date_en)  # â† ì—¬ê¸°
        query_en = " ".join(parts_en).strip() or None

        # KO: ë°œí™”ì + ë‚ ì§œ
        parts_ko = []
        if speaker_ko:
            parts_ko.append(str(speaker_ko))
        if article_date_str:
            parts_ko.append(article_date_str)
        query_ko = " ".join(parts_ko).strip() or None

        logger.info("[RollcallQuery] ko=%s", query_ko)
        logger.info("[RollcallQuery] en=%s", query_en)

        # ë¡¤ì½œ ëª¨ë“œì—ì„œëŠ” ì—¬ê¸°ì„œ ë°”ë¡œ ì¢…ë£Œ
        return {"ko": query_ko, "en": query_en}

    # =========================
    # 2) ì¼ë°˜ ëª¨ë“œ (ê¸°ì¡´ ë¡œì§)
    # =========================
    base_tokens: List[str] = [speaker_en] + locs_en_tokens + kws_en_tokens
    if date_en:
        base_tokens.append(date_en)   # â† ì—¬ê¸°ì„œë„ "November 30, 2025" ì¶”ê°€


    query_en_tokens: List[str] = _dedupe_preserve(
        [speaker_en] + locs_en_tokens + kws_en_tokens
    )
    if quote_en_full:
        query_en_tokens.append(quote_en_full)
    query_en = " ".join(query_en_tokens).strip()

    query_ko_parts = [speaker_ko]
    if locs_ko:
        query_ko_parts.append(locs_ko)
    if top_kws_ko:
        query_ko_parts.append(" ".join(top_kws_ko))
    if quote_sentence:
        query_ko_parts.append(quote_sentence)
    query_ko = " ".join(
        _dedupe_preserve(" ".join(query_ko_parts).split())
    ).strip()

    return {"ko": query_ko or None, "en": query_en or None}
